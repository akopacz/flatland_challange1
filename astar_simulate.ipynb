{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# In Flatland you can use custom observation builders and predicitors\n",
    "# Observation builders generate the observation needed by the controller\n",
    "# Preditctors can be used to do short time prediction which can help in avoiding conflicts in the network\n",
    "from flatland.envs.malfunction_generators import malfunction_from_params, MalfunctionProcessData # MalfunctionParameters\n",
    "from flatland.envs.observations import GlobalObsForRailEnv\n",
    "# First of all we import the Flatland rail environment\n",
    "from flatland.envs.rail_env import RailEnv\n",
    "from flatland.envs.rail_env import RailEnvActions\n",
    "from flatland.envs.rail_generators import sparse_rail_generator\n",
    "from flatland.envs.schedule_generators import sparse_schedule_generator\n",
    "# We also include a renderer because we want to visualize what is going on in the environment\n",
    "from flatland.utils.rendertools import RenderTool, AgentRenderVariant\n",
    "\n",
    "import flatland.core.grid.grid4_astar as fl_astar\n",
    "\n",
    "# This is an introduction example for the Flatland 2.1.* version.\n",
    "# Changes and highlights of this version include\n",
    "# - Stochastic events (malfunctions)\n",
    "# - Different travel speeds for differet agents\n",
    "# - Levels are generated using a novel generator to reflect more realistic railway networks\n",
    "# - Agents start outside of the environment and enter at their own time\n",
    "# - Agents leave the environment after they have reached their goal\n",
    "# Use the new sparse_rail_generator to generate feasible network configurations with corresponding tasks\n",
    "# Training on simple small tasks is the best way to get familiar with the environment\n",
    "# We start by importing the necessary rail and schedule generators\n",
    "# The rail generator will generate the railway infrastructure\n",
    "# The schedule generator will assign tasks to all the agent within the railway network\n",
    "\n",
    "# The railway infrastructure can be build using any of the provided generators in env/rail_generators.py\n",
    "# Here we use the sparse_rail_generator with the following parameters\n",
    "\n",
    "width = 16 * 4 # 16 * 7  # With of map\n",
    "height = 9 * 4 # 9 * 7  # Height of map\n",
    "nr_trains = 10 # nr_trains = 50  # Number of trains that have an assigned task in the env\n",
    "cities_in_map = 6 # 20  # Number of cities where agents can start or end\n",
    "seed = 14  # Random seed\n",
    "grid_distribution_of_cities = False  # Type of city distribution, if False cities are randomly placed\n",
    "max_rails_between_cities = 2  # Max number of tracks allowed between cities. This is number of entry point to a city\n",
    "max_rail_in_cities = 6  # Max number of parallel tracks within a city, representing a realistic trainstation\n",
    "\n",
    "rail_generator = sparse_rail_generator(max_num_cities=cities_in_map,\n",
    "                                       seed=seed,\n",
    "                                       grid_mode=grid_distribution_of_cities,\n",
    "                                       max_rails_between_cities=max_rails_between_cities,\n",
    "                                       max_rails_in_city=max_rail_in_cities,\n",
    "                                       )\n",
    "\n",
    "# The schedule generator can make very basic schedules with a start point, end point and a speed profile for each agent.\n",
    "# The speed profiles can be adjusted directly as well as shown later on. We start by introducing a statistical\n",
    "# distribution of speed profiles\n",
    "\n",
    "# Different agent types (trains) with different speeds.\n",
    "speed_ration_map = {1.: 0.25,  # Fast passenger train\n",
    "                    1. / 2.: 0.25,  # Fast freight train\n",
    "                    1. / 3.: 0.25,  # Slow commuter train\n",
    "                    1. / 4.: 0.25}  # Slow freight train\n",
    "\n",
    "# We can now initiate the schedule generator with the given speed profiles\n",
    "\n",
    "schedule_generator = sparse_schedule_generator(speed_ration_map)\n",
    "\n",
    "# We can furthermore pass stochastic data to the RailEnv constructor which will allow for stochastic malfunctions\n",
    "# during an episode.\n",
    "\n",
    "stochastic_data = { \"malfunction_rate\":10000,  # Rate of malfunction occurence\n",
    "                                        \"min_duration\":15,  # Minimal duration of malfunction\n",
    "                                        \"max_duration\":50  # Max duration of malfunction\n",
    "}\n",
    "# stochastic_data = MalfunctionParameters(malfunction_rate=10000,  # Rate of malfunction occurence\n",
    "#                                         min_duration=15,  # Minimal duration of malfunction\n",
    "#                                         max_duration=50  # Max duration of malfunction\n",
    "#                                         )\n",
    "# Custom observation builder without predictor\n",
    "observation_builder = GlobalObsForRailEnv()\n",
    "\n",
    "# Custom observation builder with predictor, uncomment line below if you want to try this one\n",
    "# observation_builder = TreeObsForRailEnv(max_depth=2, predictor=ShortestPathPredictorForRailEnv())\n",
    "\n",
    "# Construct the enviornment with the given observation, generataors, predictors, and stochastic data\n",
    "env = RailEnv(width=width,\n",
    "              height=height,\n",
    "              rail_generator=rail_generator,\n",
    "              schedule_generator=schedule_generator,\n",
    "              number_of_agents=nr_trains,\n",
    "              obs_builder_object=observation_builder,\n",
    "              malfunction_generator_and_process_data=malfunction_from_params(stochastic_data),\n",
    "              remove_agents_at_target=True)\n",
    "env.reset()\n",
    "\n",
    "# Initiate the renderer\n",
    "env_renderer = RenderTool(env, gl=\"PILSVG\",\n",
    "                          agent_render_variant=AgentRenderVariant.ONE_STEP_BEHIND,\n",
    "                          show_debug=False,\n",
    "                          screen_height=1200,  # Adjust these parameters to fit your resolution\n",
    "                          screen_width=1800)  # Adjust these parameters to fit your resolution\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The first thing we notice is that some agents don't have feasible paths to their target.\n",
    "# We first look at the map we have created\n",
    "\n",
    "# nv_renderer.render_env(show=True)\n",
    "# time.sleep(2)\n",
    "# Import your own Agent or use RLlib to train agents on Flatland\n",
    "from greedy_agent import GreedyAgent\n",
    "\n",
    "import build_graph as graph_builder\n",
    "import networkx as nx\n",
    "\n",
    "# Initialize the agent with the parameters corresponding to the environment and observation_builder\n",
    "controller = GreedyAgent(218, env.action_space[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by looking at the information of each agent\n",
    "# We can see the task assigned to the agent by looking at\n",
    "print(\"\\n Agents in the environment have to solve the following tasks: \\n\")\n",
    "for agent_idx, agent in enumerate(env.agents):\n",
    "    print(\n",
    "        \"The agent with index {} has the task to go from its initial position {}, facing in the direction {} to its target at {}.\".format(\n",
    "            agent_idx, agent.initial_position, agent.direction, agent.target))\n",
    "\n",
    "# The agent will always have a status indicating if it is currently present in the environment or done or active\n",
    "# For example we see that agent with index 0 is currently not active\n",
    "print(\"\\n Their current statuses are:\")\n",
    "print(\"============================\")\n",
    "\n",
    "for agent_idx, agent in enumerate(env.agents):\n",
    "    print(\"Agent {} status is: {} with its current position being {}\".format(agent_idx, str(agent.status), str(agent.position)))\n",
    "\n",
    "# The agent needs to take any action [1,2,3] except do_nothing or stop to enter the level\n",
    "# If the starting cell is free they will enter the level\n",
    "# If multiple agents want to enter the same cell at the same time the lower index agent will enter first.\n",
    "\n",
    "# Let's check if there are any agents with the same start location\n",
    "agents_with_same_start = set()\n",
    "print(\"\\n The following agents have the same initial position:\")\n",
    "print(\"=====================================================\")\n",
    "for agent_idx, agent in enumerate(env.agents):\n",
    "    for agent_2_idx, agent2 in enumerate(env.agents):\n",
    "        if agent_idx != agent_2_idx and agent.initial_position == agent2.initial_position:\n",
    "            print(\"Agent {} as the same initial position as agent {}\".format(agent_idx, agent_2_idx))\n",
    "            agents_with_same_start.add(agent_idx)\n",
    "\n",
    "# Lets try to enter with all of these agents at the same time\n",
    "action_dict = dict()\n",
    "\n",
    "for agent_id in agents_with_same_start:\n",
    "    action_dict[agent_id] = 1  # Try to move with the agents\n",
    "\n",
    "# Do a step in the environment to see what agents entered:\n",
    "env.step(action_dict)\n",
    "\n",
    "# Current state and position of the agents after all agents with same start position tried to move\n",
    "print(\"\\n This happened when all tried to enter at the same time:\")\n",
    "print(\"========================================================\")\n",
    "for agent_id in agents_with_same_start:\n",
    "    print(\n",
    "        \"Agent {} status is: {} with the current position being {}.\".format(\n",
    "            agent_id, str(env.agents[agent_id].status),\n",
    "            str(env.agents[agent_id].position)))\n",
    "\n",
    "# As you see only the agents with lower indexes moved. As soon as the cell is free again the agents can attempt\n",
    "# to start again.\n",
    "\n",
    "# You will also notice, that the agents move at different speeds once they are on the rail.\n",
    "# The agents will always move at full speed when moving, never a speed inbetween.\n",
    "# The fastest an agent can go is 1, meaning that it moves to the next cell at every time step\n",
    "# All slower speeds indicate the fraction of a cell that is moved at each time step\n",
    "# Lets look at the current speed data of the agents:\n",
    "\n",
    "print(\"\\n The speed information of the agents are:\")\n",
    "print(\"=========================================\")\n",
    "\n",
    "for agent_idx, agent in enumerate(env.agents):\n",
    "    print(\n",
    "        \"Agent {} speed is: {:.2f} with the current fractional position being {}\".format(\n",
    "            agent_idx, agent.speed_data['speed'], agent.speed_data['position_fraction']))\n",
    "\n",
    "# New the agents can also have stochastic malfunctions happening which will lead to them being unable to move\n",
    "# for a certain amount of time steps. The malfunction data of the agents can easily be accessed as follows\n",
    "print(\"\\n The malfunction data of the agents are:\")\n",
    "print(\"========================================\")\n",
    "\n",
    "for agent_idx, agent in enumerate(env.agents):\n",
    "    print(\n",
    "        \"Agent {} is OK = {}\".format(\n",
    "            agent_idx, agent.malfunction_data['malfunction'] < 1))\n",
    "\n",
    "# Now that you have seen these novel concepts that were introduced you will realize that agents don't need to take\n",
    "# an action at every time step as it will only change the outcome when actions are chosen at cell entry.\n",
    "# Therefore the environment provides information about what agents need to provide an action in the next step.\n",
    "# You can access this in the following way.\n",
    "\n",
    "# Chose an action for each agent\n",
    "for a in range(env.get_num_agents()):\n",
    "    action = controller.act(0)\n",
    "    action_dict.update({a: action})\n",
    "# Do the environment step\n",
    "observations, rewards, dones, information = env.step(action_dict)\n",
    "print(\"\\n The following agents can register an action:\")\n",
    "print(\"========================================\")\n",
    "for info in information['action_required']:\n",
    "    print(\"Agent {} needs to submit an action.\".format(info))\n",
    "\n",
    "# We recommend that you monitor the malfunction data and the action required in order to optimize your training\n",
    "# and controlling code.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create whitelist, which cells not to remove from graph\n",
    "cell_whitelist = set()\n",
    "for ag in env.agents:\n",
    "    start = graph_builder.convert_indexes_2_node(ag.initial_position, env.rail.width)\n",
    "    if ag.position is not None:\n",
    "        start = graph_builder.convert_indexes_2_node(ag.position, env.rail.width)\n",
    "    cell_whitelist.update([start, graph_builder.convert_indexes_2_node(ag.target, env.rail.width)])\n",
    "\n",
    "print(\"white list:\", cell_whitelist)\n",
    "\n",
    "# Build graph from transition map\n",
    "print(\"\\nCompute transition graph from generated rail grid.\")\n",
    "trs = graph_builder.grid2cells(env.rail)\n",
    "g = graph_builder.graph_from_cell_neighbors(trs, env.rail.width, env.rail.height, whitelist=cell_whitelist)\n",
    "print(g.number_of_nodes(), g.number_of_edges(), \"\\n\")\n",
    "\n",
    "astar_paths = []\n",
    "astar_paths_readable = []\n",
    "# run A* for the agents\n",
    "for ag in env.agents:\n",
    "    start = graph_builder.convert_indexes_2_node(ag.initial_position, env.rail.width)\n",
    "    if ag.position is not None:\n",
    "        start = graph_builder.convert_indexes_2_node(ag.position, env.rail.width)\n",
    "    end = graph_builder.convert_indexes_2_node(ag.target, env.rail.width)\n",
    "    astar_paths.append(nx.astar_path(g, start, end))\n",
    "    astar_paths_readable.append([graph_builder.convert_node_2_indexes(node, env.rail.width) for node in astar_paths[-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flatland.envs.agent_utils import RailAgentStatus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us now look at an episode playing out with random actions performed\n",
    "print(\"\\nStart episode...\")\n",
    "\n",
    "# Reset the rendering system\n",
    "env_renderer.reset()\n",
    "\n",
    "# Here you can also further enhance the provided observation by means of normalization\n",
    "# See training navigation example in the baseline repository\n",
    "\n",
    "\n",
    "score = 0\n",
    "# Run episode\n",
    "frame_step = 0\n",
    "\n",
    "agent_left_node = np.ones(len(env.agents), dtype=int)\n",
    "# check if agents are already departed\n",
    "# for a_id, ag in enumerate(env.agents):\n",
    "#     pass\n",
    "\n",
    "for step in range(100): # range(500):\n",
    "    # Chose an action for each agent in the environment\n",
    "    for a_id, ag in enumerate(env.agents):\n",
    "        if ag.status == RailAgentStatus.ACTIVE:\n",
    "            # follow path defined by a*\n",
    "            if ag.position == astar_paths_readable[a_id][agent_left_node[a_id]]:\n",
    "                # arrived at a (graph node) possible intersection\n",
    "                agent_left_node[a_id] += 1\n",
    "                next_node = astar_paths[a_id][agent_left_node[a_id]]\n",
    "                # decide which way to go next\n",
    "                from_ = astar_paths[a_id][agent_left_node[a_id]-1]\n",
    "                dirs = g[from_][next_node]\n",
    "                if dirs[\"dir0\"] == ag.direction:\n",
    "                    action = controller.change_dir_from_to(dirs[\"dir0\"], dirs[\"dir1\"])\n",
    "                    print(f\"Agent {a_id} changed its direction from {ag.direction} to {dirs['dir1']}\")\n",
    "                else:\n",
    "                    action = controller.change_dir_from_to(dirs[\"dir1\"], dirs[\"dir0\"])\n",
    "                    print(f\"Agent {a_id} changed its direction from {ag.direction} to {dirs['dir0']}\")\n",
    "            else:\n",
    "                # go forward... or check where should go\n",
    "                action = 2\n",
    "\n",
    "            action_dict.update({a_id: action})\n",
    "        elif ag.status == RailAgentStatus.READY_TO_DEPART:\n",
    "            # initializing with a going forward movement\n",
    "            action_dict.update({a_id: 2})\n",
    "\n",
    "    # Environment step which returns the observations for all agents, their corresponding\n",
    "    # reward and whether their are done\n",
    "\n",
    "    next_obs, all_rewards, done, _ = env.step(action_dict)\n",
    "\n",
    "    env_renderer.render_env(show=True, show_observations=False, show_predictions=False)\n",
    "    env_renderer.gl.save_image('../misc/Fames2/flatland_frame_{:04d}.png'.format(step))\n",
    "    frame_step += 1\n",
    "    # Update replay buffer and train agent\n",
    "    for a in range(env.get_num_agents()):\n",
    "        controller.step((observations[a], action_dict[a], all_rewards[a], next_obs[a], done[a]))\n",
    "        score += all_rewards[a]\n",
    "\n",
    "    observations = next_obs.copy()\n",
    "    if done['__all__']:\n",
    "        break\n",
    "    print('Episode: Steps {}\\t Score = {}'.format(step, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
